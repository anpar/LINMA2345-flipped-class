\begin{frame}{Learning rules}
    \textbf{Learning strategy}
    \begin{itemize}
        \item Use of previous information to improve its behavior
        \item Strategies that as they unfold draw interesting inferences or use accumulate experience in an interesting way
        \item Two types of learning will be covered: \textbf{Fictitious play} and \textbf{No-regret playing}
    \end{itemize}
\end{frame}

\begin{comment}
\begin{frame}{Learning rules}
    \textbf{Two types of theories}
    \begin{itemize}
        \item Descriptive theories: theories that attempt to study the way learning takes place in real life
        \item Prescriptive theories: theories that study the way agents \textit{should} react in real life
    \end{itemize}
\end{frame}

\begin{frame}{Descriptive theories}
    \textbf{Properties}
    \begin{itemize}
        \item \textbf{Realism}: There should be a good match between the formal theory and the natural phenomenon being studied
        \item \textbf{Convergence}: The theory should exhibit convergence of the strategy profile to some equilibrium
    \end{itemize}
\end{frame}

\begin{frame}{Prescriptive theories}
    \textbf{Properties}
    \begin{itemize}
        \item \textbf{Safety}: Guarantees the agent at least learning rule its maxmin payoff, or “security value.”
        \item \textbf{Rationality}: Whenever the opponent's learning rule settles on a stationary strategy the agent settles on a best response to that strategy
        \item \textbf{No-regret}: Yields a better result that any pure strategy the agent could have played
    \end{itemize}
\end{frame}
\end{comment}
\begin{frame}{Fictitious play}
    \textbf{Fictitious play}
    \begin{itemize}
        \item One of the earliest learning rules 
        \item Instance of model-based learning where learner maintains belief about opponent
        \item Assumest that opponent is playing mixed strategy given by empirical distribution of opponent’s previous actions
        \item Doesn't necessarily converge
    \end{itemize}
\end{frame}

\begin{frame}{Fictitious play}
    \begin{algorithm}[H]
         Initialize beliefs about the opponent strategy\;
         \While{true}{
            Play a best response to the assessed strategy of the opponent\\
            Observe the opponent’s actual play and update beliefs accordingly\\
         }
    \caption{Fictitious play algorithm}
    \end{algorithm}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{Example}
        Consider the following game.
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
    \end{exampleblock}
\end{frame}

\note{
    $\nu_i^t$:belief of player $i$ at turn $t$,$\mu_i^t$:expected play of player $i$ at turn $t$,$\sigma_i^t$:strategy of player $i$ at turn $t+1$\\
    Let's assume that player 1 plays U and player 2 plays l at first turn.\\
    \begin{itemize}
        \item 2: $\nu_1^2=(1,0), \nu_2^2=(1,0)$, ep: $\mu_1^2=l$, $\mu_2^2=U$, s: $\sigma_1^2=D$, $\sigma_2^2=l$ 
        \item 3: $\nu_1^3=(2,0), \nu_2^3=(1,1)$, ep: $\mu_1^3=l$ , $\mu_2^3=\frac{1}{2}U+\frac{1}{2}D$ , s: $\sigma_1^3=D$, $\sigma_2^3=l$ 
        \item 4: $\nu_1^4=(3,0), \nu_2^4=(1,2)$, ep: $\mu_1^4=l$ , $\mu_2^4=\frac{1}{3}U+\frac{2}{3}D$, s: $\sigma_1^4=D$, $\sigma_2^4=l$ (expected payoff: $l:\frac{1}{3}*3+\frac{2}{3}*0=1$,$r:\frac{1}{3}*0+\frac{2}{3}*1=\frac{2}{3}$) 
        \item 5: $\nu_1^5=(4,0), \nu_2^5=(1,3)$, ep: $\mu_1^5=l$ , $\mu_2^5=\frac{1}{4}U+\frac{3}{4}D$, s: $\sigma_1^5=D$, $\sigma_2^5=l$
        \item 6: $\nu_1^6=(5,0), \nu_2^6=(1,4)$, ep: $\mu_1^6=l$ , $\mu_2^6=\frac{1}{5}U+\frac{4}{5}D$, s: $\sigma_1^6=D$, $\sigma_2^6=r$
    \end{itemize}
}



\begin{frame}{No-regret playing}
    \textbf{Regret}\\
    \textit{Let $\alpha^t$ be the average per-period reward the agent received up to time $t$ and $\alpha^t(s)$ the average per-period reward the agent would have received by playing strategy $s$.}\\
    \textit{The regret an agent experiences at time $t$ for not having played $s$ is $R^t(s)=\alpha^t-\alpha^t(s)$}
\end{frame}

\begin{frame}{No-regret playing}
    \textbf{No-regret learning rule}\\
    \textit{A learning rule exhibits \textbf{no regret} if it guarantees with high probability that the agent will not experience any positive regret}
\end{frame}

\begin{frame}{No-regret playing}
    \textbf{Example of a no-regret strategy: Regret matching}\\
    \begin{itemize}
        \item At each time step each action is chosen with probability consistent to its regret
        \item The strategy of playing strategy $s$ at timestep $t+1$ is equal to:
        \[
            \sigma_i^{t+1}(s)= \frac{R^t(s)}{\sum_{s'\in S_i} R^t(s')}
        \]
    \end{itemize}
\end{frame}

\begin{frame}{No-regret playing}
    \begin{exampleblock}{Example}
        Consider the battle of sexes game.
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}t}    & {\color{red}f} \\
                \hline
                {\color{green}T}    & \payoff{3}{2}   & \payoff{1}{~1} \\
                {\color{green}F}    & \payoff{~0}{0}    & \payoff{2}{3} 
            \end{tabular}
            \caption{Battle of sexes}
        \end{table}
    \end{exampleblock}
\end{frame}

\note{
    Let's assume that player 1 plays T and player 2 plays f at first turn.\\
    \begin{itemize}
        \item 2: $\alpha_1^1=1, \alpha_1^1(T)=1, \alpha_1^1(F)=2, R_1^1(T)=0, R_1^1(F)=-1, \sigma_1^2(T)=\frac{0}{0-1}=0, \sigma_1^2(F)=\frac{-1}{0-1}=1 \rightarrow$ plays F\\
                 $\alpha_2^1=1, \alpha_2^1(t)=2, \alpha_2^1(f)=1, R_1^1(t)=-1, R_1^1(f)=0, \sigma_2^2(t)=\frac{-1}{0-1}=1, \sigma_1^2(F)=\frac{0}{0-1}=0
        \rightarrow$ plays t
        \item 3: $\alpha_1^2=0.5, \alpha_1^2(T)=\frac{1+3}{2}=2, \alpha_1^2(F)=\frac{2+0}{2}=1, R_1^2(T)=-1.5, R_1^2(F)=-0.5, \sigma_1^2(T)=\frac{-1.5}{-2}=0.75, \sigma_1^2(F)=\frac{-0.5}{-2}=0.25 \rightarrow$ plays 0.75[T]+0.25[F]\\
                 $\alpha_2^2=0.5, \alpha_2^2(t)=\frac{2+0}{2}=1, \alpha_2^2(f)=\frac{1+3}{2}=2, R_2^2(t)=-0.5, R_2^2(f)=-1.5, \sigma_2^2(t)=\frac{-0.5}{-2}=0.25, \sigma_2^2(f)=\frac{-1.5}{-2}=0.75 \rightarrow$ plays 0.25[t]+0.75[f]\\
    \end{itemize}
}

\begin{frame}{Take-home message \#3}
    \metroset{block=fill}
    \begin{block}{Take-home message \#3}
        Fictitious play algorithm: assume that opponent play mixed strategy and respond by best pure strategy\\
        No-regret playing: $P([\liminf R^t(s)]\leq 0)=1$\\
        Regret matching: play mixed strategy with probability consistent with regret
    \end{block}
\end{frame}
