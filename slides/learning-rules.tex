\begin{frame}{Learning strategy}
    \begin{exampleblock}{What is a learning strategy?}
        Learning strategies are strategies that
        \begin{itemize}
            \item use previous information to improve its behavior
            \item as they unfold \textbf{draw interesting inferences or use accumulate experience
            in an interesting way}
        \end{itemize}
    \end{exampleblock}

    \vspace{0.5cm}
    \begin{block}{Tow learning strategies today}
        \begin{enumerate}
            \item \textit{Fictitious play}
            \item \textit{No-regret playing}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Fictitious play}
    \begin{block}{Fictitious play}
        \begin{itemize}
            \item Introduced by \textsc{Brown} at RAND (again) in 1949 as an explanation to Nash
            equilibrium
            \item Instance of \textbf{model-based} learning where learner maintains belief about opponent
            \item Assumes that opponent is playing mixed strategy given by empirical distribution of
            opponent’s previous actions
            \item Doesn't necessarily converge
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Fictitious play: algorithm}
    \begin{algorithm}[H]
         \While{true}{
            \If{beliefs about the opponent strategy is empty}{
                play an arbitrary (not necessarily rational) strategy 
            }
            \Else{
                play a best response to the assessed strategy of the opponent
            }
            observe the opponent’s actual play and update beliefs accordingly
         }
    \caption{Fictitious play algorithm}
    \end{algorithm}
\end{frame}

\begin{frame}{Fictitious Play: example (1)}
    \begin{exampleblock}{Example}
        Consider the following game.
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Normal form game used in the Fictitious play example. The only
            Nash Equilibrium of this game us ({\color{green}D}, {\color{red}r}).}
        \end{table}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play: example (2)}
    \begin{exampleblock}{Let's use the following notations}
        \begin{itemize}
            \item $\nu_i^t$: belief of player $i$ at turn $t$
            \item $\mu_i^t$: expected play of player $i$ at turn $t$
            \item $\sigma_i^t$: strategy of player $i$ at turn $t+1$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{1st turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        Let's assume that player 1 plays U and player 2 plays l at first turn.\\
        (worst case, we want to prove convergence for every initial condition)\\
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{2nd turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        \begin{itemize}
            \item Belief: $\nu_1^2=(1,0), \nu_2^2=(1,0)$
            \item Expected play: $\mu_1^2=l$, $\mu_2^2=U$
            \item Strategy: $\sigma_1^2=D$, $\sigma_2^2=l$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{3rd turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        \begin{itemize}
            \item Belief: $\nu_1^3=(2,0), \nu_2^3=(1,1)$
            \item Expected play: $\mu_1^3=l$ , $\mu_2^3=\frac{1}{2}U+\frac{1}{2}D$
            \item Strategy: $\sigma_1^3=D$, $\sigma_2^3=l$\\ (expected payoff: $l:\frac{1}{2}*3+\frac{1}{2}*0=1.5$,$r:\frac{1}{2}*0+\frac{1}{2}*1=0.5$)
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{4th turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        \begin{itemize}
            \item Belief: $\nu_1^4=(3,0), \nu_2^4=(1,2)$
            \item Expected play: $\mu_1^4=l$ , $\mu_2^4=\frac{1}{3}U+\frac{2}{3}D$
            \item Strategy: $\sigma_1^4=D$, $\sigma_2^4=l$\\ (expected payoff: $l:\frac{1}{3}*3+\frac{2}{3}*0=1$,$r:\frac{1}{3}*0+\frac{2}{3}*1=\frac{2}{3}$)
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{5th turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        \begin{itemize}
            \item Belief: $\nu_1^5=(4,0), \nu_2^5=(1,3)$
            \item Expected play: $\mu_1^5=l$ , $\mu_2^5=\frac{1}{4}U+\frac{3}{4}D$
            \item Strategy: $\sigma_1^5=D$, $\sigma_2^5=l$\\ (expected payoff: $l:\frac{1}{4}*3+\frac{3}{4}*0=0.75$,$r:\frac{1}{4}*0+\frac{3}{4}*1=0.75$)
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Fictitious Play}
    \begin{exampleblock}{6th turn}
        \begin{table}
            \begin{tabular}{c|cc}
                                    & {\color{red}l}    & {\color{red}r} \\
                \hline
                {\color{green}U}    & \payoff{3}{3}   & \payoff{0}{~0} \\
                {\color{green}D}    & \payoff{~4}{0}    & \payoff{1}{1} 
            \end{tabular}
            \caption{Example}
        \end{table}
        \begin{itemize}
            \item Belief: $\nu_1^6=(5,0), \nu_2^6=(1,4)$
            \item Expected play: $\mu_1^6=l$ , $\mu_2^6=\frac{1}{5}U+\frac{4}{5}D$
            \item Strategy: $\sigma_1^6=D$, $\sigma_2^6=r$\\ (expected payoff: $l:\frac{1}{5}*3+\frac{4}{5}*0=0.6$,$r:\frac{1}{5}*0+\frac{4}{5}*1=0.8$)
        \end{itemize}
    \end{exampleblock}
\end{frame}


\begin{frame}{Convergence of Fictitious play}
    \metroset{block=fill}

    \begin{block}{Theorem}
        If the empirical distribution of each player's strategies converges
        in fictitious play, then \textbf{it converges to a Nash equilibrium}.
    \end{block}

    \begin{block}{Sufficient conditions for convergence}
        Each of the following are a sufficient conditions for the empirical
        frequencies of play to converge in fictitious play
        \begin{itemize}
            \item the game is zero-sum;
            \item the game is solvable by iterated elimination of strictly dominated strategies
            \item {\color{gray}the game is a potential game}
            \item {\color{gray}the game is $2\times n$ and has generic payoffs}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{No-regret playing}
    Let $\alpha^t$ be the average per-period reward the agent received up to time $t$
    and $\alpha^t(d)$ the average per-period reward the agent would have received by playing
    strategy $d$. \\
    
    \metroset{block=fill}
    \begin{block}{Regret}
        The regret an agent experiences at time $t$ for not having played $d$ is
        $$R^t(d) = \alpha^t(d) - \alpha^t.$$
    \end{block}
\end{frame}

\begin{frame}{No-regret playing}
    \metroset{block=fill}
    \begin{block}{No-regret playing}
        A learning rule exhibits \textbf{no regret} if it guarantees with high probability that the
        agent will not experience any positive regret
        $$P([\liminf R^t(d)] \leq 0) = 1.$$
    \end{block}
\end{frame}

\begin{frame}{Example: regret matching}
    \begin{block}{Regret matching}
        \begin{itemize}
            \item At each time step each action is chosen with probability consistent to its regret
            \item The probability of playing strategy $d$ at time-step $t+1$ is equal to:
            \[
                \sigma_i^{t+1}(d)= \frac{R^t(d)}{\sum_{d'\in D_i} R^t(d')}.
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Take-home message \#8}
    \metroset{block=fill}
    \begin{block}{Take-home message \#8}
        \begin{itemize}
            \item Learning strategies can be \textbf{model-based} (e.g. fictitious play) or not.
            \item In fictitious play, {\color{green}strategies empirical distribution converges to a
            Nash equilibrium} (if it converges).
            \item No-regret playing: $P([\liminf R^t(d)] \leq 0) = 1$.
            \item Regret matching: play mixed strategy with probability consistent with regret.
        \end{itemize}
    \end{block}
\end{frame}
